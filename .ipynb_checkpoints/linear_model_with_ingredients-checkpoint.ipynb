{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "import re\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://andylane@localhost/restaurants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/sqlalchemy/dialects/__init__.py:34: SADeprecationWarning: The SQLAlchemy PostgreSQL dialect has been renamed from 'postgres' to 'postgresql'. The new URL format is postgresql[+driver]://<user>:<pass>@<host>/<dbname>\n",
      "  module = __import__('sqlalchemy.dialects.%s' % (dialect, )).dialects\n"
     ]
    }
   ],
   "source": [
    "dbname = 'restaurants'\n",
    "username = 'andylane'\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)\n",
    "con = None\n",
    "con = psycopg2.connect(database = \"restaurants\", user = \"andylane\")\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from menusights_aux import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipes = pd.read_sql(\"recipes\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recipe names into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>calories</th>\n",
       "      <th>fat</th>\n",
       "      <th>carbs</th>\n",
       "      <th>protein</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>sodium</th>\n",
       "      <th>servings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Fluffy Pancakes</td>\n",
       "      <td>http://allrecipes.com/recipe/162760/fluffy-pan...</td>\n",
       "      <td>230</td>\n",
       "      <td>8.2</td>\n",
       "      <td>32.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>65.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Joan's Quick Chili</td>\n",
       "      <td>http://allrecipes.com/recipe/240622/joans-quic...</td>\n",
       "      <td>397</td>\n",
       "      <td>21.5</td>\n",
       "      <td>25.2</td>\n",
       "      <td>26.7</td>\n",
       "      <td>71.0</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ultimate Double Chocolate Cookies</td>\n",
       "      <td>http://allrecipes.com/recipe/15097/ultimate-do...</td>\n",
       "      <td>148</td>\n",
       "      <td>6.8</td>\n",
       "      <td>21.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Campfire Banana Splits</td>\n",
       "      <td>http://allrecipes.com/recipe/20038/campfire-ba...</td>\n",
       "      <td>545</td>\n",
       "      <td>17.3</td>\n",
       "      <td>106.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Pumpkin Chocolate Chip Cookies II</td>\n",
       "      <td>http://allrecipes.com/recipe/10117/pumpkin-cho...</td>\n",
       "      <td>224</td>\n",
       "      <td>11.5</td>\n",
       "      <td>29.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                               name  \\\n",
       "0   1                    Fluffy Pancakes   \n",
       "1   2                 Joan's Quick Chili   \n",
       "2   3  Ultimate Double Chocolate Cookies   \n",
       "3   4             Campfire Banana Splits   \n",
       "4   5  Pumpkin Chocolate Chip Cookies II   \n",
       "\n",
       "                                                 url  calories   fat  carbs  \\\n",
       "0  http://allrecipes.com/recipe/162760/fluffy-pan...       230   8.2   32.7   \n",
       "1  http://allrecipes.com/recipe/240622/joans-quic...       397  21.5   25.2   \n",
       "2  http://allrecipes.com/recipe/15097/ultimate-do...       148   6.8   21.4   \n",
       "3  http://allrecipes.com/recipe/20038/campfire-ba...       545  17.3  106.2   \n",
       "4  http://allrecipes.com/recipe/10117/pumpkin-cho...       224  11.5   29.5   \n",
       "\n",
       "   protein  cholesterol  sodium  servings  \n",
       "0      6.4         65.0   650.0         4  \n",
       "1     26.7         71.0  1438.0         4  \n",
       "2      2.2         25.0    88.0        42  \n",
       "3      4.7          0.0    47.0         6  \n",
       "4      2.8         10.0   127.0        18  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_up_ingredient(ingredient_line):\n",
    "    ingredient_line = re.sub(\"\\[u\\'\", \"\", ingredient_line)\n",
    "    ingredient_line = re.sub(\"\\']\", \"\", ingredient_line)\n",
    "    return find_measurement_words(ingredient_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Query' object has no attribute 'Recipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a8582c6aef1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRecipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mingredients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Query' object has no attribute 'Recipe'"
     ]
    }
   ],
   "source": [
    "names = session.query(Recipe).Recipe.ingredients.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iterrorws'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bd1e630fa1e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecipes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrorws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2670\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2671\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2672\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iterrorws'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot distribution of cholesterol etc; looks like it's normalish\n",
    "from sklearn.preprocessing import scale\n",
    "def normalize(item):\n",
    "    a = scale(recipes[item], axis=0, with_mean=True, with_std=True, copy=True )\n",
    "    return a\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.distplot(normalize(\"cholesterol\"), label=\"chol\")\n",
    "#ax = sns.distplot(normalize(\"calories\"))\n",
    "#ax = sns.distplot(normalize(\"protein\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(title):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_titles = []\n",
    "    new_title=[]\n",
    "    for word in nltk.word_tokenize(title):\n",
    "        new_title.append(stemmer.stem(word))\n",
    "    stemmed_titles.extend(new_title)\n",
    "    return \" \".join([i for i in stemmed_titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_names = [tokenize_and_stem(i) for i in (recipes[\"name\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_names[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3), min_df=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = vectorizer.fit_transform(tokenized_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_.get('minc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()[2256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(a.toarray()[:,2701])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression, mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# produces an array with mutual information (weights?) between individual words/n-grams in recipe names and cholesterol information\n",
    "mi = mutual_info_regression(a, recipes[\"cholesterol\"])\n",
    "mi /= np.max(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#[(index, i, vectorizer.get_feature_names()[index]) for index, i in enumerate(mi) if i>0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns indices of columns with MI value greater than e.g. 0.01. Can be used for re\n",
    "informative_words = np.array([(index) for index, i in enumerate(mi) if i>0.01])\n",
    "# return word vector array with uninformative words removeds\n",
    "culled_array = a.toarray()[:,informative_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "culled_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "X_train = culled_array\n",
    "y_train = recipes[\"cholesterol\"]\n",
    "\n",
    "X_test = a[:200]\n",
    "y_test = recipes[\"cholesterol\"][:200]\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try fitting an elasticnet model!\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "enet = ElasticNet()\n",
    "enetmodel=enet.fit(culled_array[200:], y_train[200:])\n",
    "y_predicted = enetmodel.predict(culled_array[:200])\n",
    "# Evaluate by comparing predicted cholesterol to actual cholesterol\n",
    "r2_score_enet = r2_score(y_test, y_predicted)\n",
    "r2_score_enet #oh no it's -0.29..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth_x =[]\n",
    "ground_truth_y =[]\n",
    "for item in pkl.load(open(\"groundtruth.pkl\", \"rb\")):\n",
    "    ground_truth_x.append(item[\"item_name\"])\n",
    "    ground_truth_y.append(item[\"nf_cholesterol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_names_ground_truth = [tokenize_and_stem(i) for i in (ground_truth_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Puts words from new ground truth set into matrix from training set\n",
    "ground_truth_vectorized = vectorizer.transform(tokenized_names_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ground_truth_vectorized_culled_array = ground_truth_vectorized.toarray()[:,informative_words]\n",
    "y_predicted = enetmodel.predict(ground_truth_vectorized_culled_array)\n",
    "r2_score_enet = r2_score(ground_truth_y, y_predicted)\n",
    "r2_score_enet #oh no it's still bad..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = pd.Series(ground_truth_y, name=\"x_var\"), pd.Series(y_predicted, name=\"y_var\")\n",
    "ax = sns.regplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, the elasticnet model (a linear regression) has poor predictive power at default parameters. Can we optimize the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Compute paths\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import time, copy\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def enetcv(culled_array, y_train, ground_truth_vectorized_culled_array, ground_truth_y):\n",
    "    #culledarrayecv = np.sqrt(np.sum(culledarrayecv ** 2, axis=0)) / culledarrayecv\n",
    "    model = ElasticNetCV(cv=100).fit(culled_array, y_train)\n",
    "    m_log_alphas = -np.log10(model.alphas_)\n",
    "    y_predicted_cv = model.predict(ground_truth_vectorized_culled_array)\n",
    "    x,y = pd.Series(ground_truth_y, name=\"actual cholesterol\"), pd.Series(y_predicted_cv, name=\"predicted cholesterol\")\n",
    "    ax = sns.regplot(x=x, y=y)\n",
    "    return r2_score(ground_truth_y, y_predicted_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enetcv(a, y_train, ground_truth_vectorized, ground_truth_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=x, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## woohoo with an optimized elasticnet model, there's a 0.13705 Rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv)\n",
    "plt.axis('tight')\n",
    "plt.ylim(ymin, ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about if I try to add position-weights to words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem_with_position_weights(title):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_titles = []\n",
    "    new_title=[]\n",
    "    for index, word in enumerate(nltk.word_tokenize(title)):\n",
    "        rindex = abs(index - len(nltk.word_tokenize(title)))\n",
    "        new_title.append(str(stemmer.stem(word).encode(\"utf-8\") + str(rindex)))\n",
    "    stemmed_titles.extend(new_title)    \n",
    "    return \" \".join([i for i in stemmed_titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_names_pos = [tokenize_and_stem_with_position_weights(i) for i in (recipes[\"name\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,3), min_df=0.0003)\n",
    "a = vectorizer.fit_transform(tokenized_names_pos)\n",
    "mi = mutual_info_regression(a, recipes[\"cholesterol\"])\n",
    "mi /= np.max(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns indices of columns with MI value greater than e.g. 0.01. Can be used for re\n",
    "informative_words = np.array([(index) for index, i in enumerate(mi) if i>0.01])\n",
    "# return word vector array with uninformative words removeds\n",
    "culled_array = a.toarray()[:,informative_words]\n",
    "\n",
    "X_train = culled_array\n",
    "y_train = recipes[\"cholesterol\"]\n",
    "\n",
    "X_test = culled_array[:200]\n",
    "y_test = recipes[\"cholesterol\"][:200]\n",
    "\n",
    "# Try fitting an elasticnet model!\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "enet = ElasticNet()\n",
    "enetmodel=enet.fit(culled_array, y_train)\n",
    "y_predicted = enetmodel.predict(X_test)\n",
    "# Evaluate by comparing predicted cholesterol to actual cholesterol\n",
    "r2_score_enet = r2_score(y_test, y_predicted)\n",
    "r2_score_enet #oh no it's -0.29...\n",
    "print(r2_score_enet)\n",
    "\n",
    "# Validate against ground_truth\n",
    "tokenized_names_ground_truth = [tokenize_and_stem_with_position_weights(i) for i in (ground_truth_x)]\n",
    "ground_truth_vectorized = vectorizer.transform(tokenized_names_ground_truth)\n",
    "ground_truth_vectorized_culled_array = ground_truth_vectorized.toarray()[:,informative_words]\n",
    "y_predicted = enetmodel.predict(ground_truth_vectorized_culled_array)\n",
    "r2_score_enet = r2_score(ground_truth_y, y_predicted)\n",
    "r2_score_enet #oh no it's still bad...\n",
    "print(r2_score_enet)\n",
    "\n",
    "#Try optimizing ENet (validated across ground truth)\n",
    "model = ElasticNetCV(cv=20).fit(culled_array, y_train)\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "y_predicted_cv = model.predict(ground_truth_vectorized_culled_array)\n",
    "r2_score_enet = r2_score(ground_truth_y, y_predicted_cv)\n",
    "print(r2_score_enet) #0.056..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Hmm. The ElasticNet CV optimization still maxes out at 0.056"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Ridge(alpha = .5)\n",
    "rrfit = reg.fit(culled_array, recipes[\"cholesterol\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted = rrfit.predict(ground_truth_vectorized_culled_array)\n",
    "# Evaluate by comparing predicted cholesterol to actual cholesterol\n",
    "r2_score_rrfit = r2_score(ground_truth_y, y_predicted)\n",
    "print(r2_score_rrfit) # -0.009 - not great still "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = linear_model.RidgeCV(cv=100).fit(culled_array, recipes[\"cholesterol\"])\n",
    "m_log_alphas = -np.log10(model.alpha_)\n",
    "y_predicted_cv = model.predict(ground_truth_vectorized_culled_array)\n",
    "r2_score_rrfitcv = r2_score(ground_truth_y, y_predicted_cv)\n",
    "print(r2_score_rrfitcv) #0.056... or 0.119 with 100cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [insight]",
   "language": "python",
   "name": "Python [insight]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
