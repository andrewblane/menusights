{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andylane/Envs/tf/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import _pickle as pkl\n",
    "import random\n",
    "import pandas\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import *\n",
    "# from sklearn.linear_model import *\n",
    "# from sklearn.metrics import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib import learn\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-2c8ba478ef93>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-2c8ba478ef93>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    X =\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)\n",
    "\n",
    "X = pkl.load(open(\"tf_data/training_data_names_unstemmed.pkl\", \"rb\"))\n",
    "y = pkl.load(open(\"tf_data/training_labels_num.pkl\", \"rb\"))\n",
    "\n",
    "ground_truth_X = \n",
    "ground_truth_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campfire Banana Splits\n",
      "campfir banana split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47891898721048543"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes to self:\n",
    "\n",
    "## There are 300,000 words in the Google News model\n",
    "len(model.vocab)\n",
    "\n",
    "## Get words from given index\n",
    "model.wv.index2word[2898]\n",
    "# Out: 'judges\"\n",
    "\n",
    "## Find the odd word out:\n",
    "model.doesnt_match(X[3].split())\n",
    "\n",
    "## Access recipes:\n",
    "print(X[3])\n",
    "print(words[3])\n",
    "\n",
    "## Get the similarity between two phrases. This is implemented in gensim as\n",
    "## just the cosine similarity of the means of the two vectors\n",
    "model.n_similarity(X[3].split(), X[7].split())\n",
    "\n",
    "\n",
    "## There's a case for n-gramming with underscores:\n",
    "model.most_similar([\"cinnamon\", \"roll\"])\n",
    "# Out e.g.: coconut_flakes, graham_cracker_crumbs\n",
    "model.most_similar(\"cinnamon_roll\")\n",
    "# Out e.g.: cinnamon_rolls, oatmeal_cookie, peach_pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coconut_flakes', 0.6064666509628296),\n",
       " ('graham_cracker_crumbs', 0.6030285954475403),\n",
       " ('melted_margarine', 0.6008464097976685),\n",
       " ('Sprinkle_evenly', 0.6000593900680542),\n",
       " ('flaked_coconut', 0.5966697931289673),\n",
       " ('fudge_swirl', 0.5889239311218262),\n",
       " ('caramel_pecan', 0.5887271761894226),\n",
       " ('nougatine', 0.5879915952682495),\n",
       " ('chopped_pecans', 0.5866116285324097),\n",
       " ('lightly_buttered', 0.5859042406082153)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cinnamon_rolls', 0.655969500541687),\n",
       " ('oatmeal_cookie', 0.62870192527771),\n",
       " ('peach_pie', 0.6216853260993958),\n",
       " ('banana_walnut', 0.6158987283706665),\n",
       " ('baked_beans_peaches', 0.6140761375427246),\n",
       " ('waffle_cone', 0.6071435809135437),\n",
       " ('fruit_cobbler', 0.6069991588592529),\n",
       " ('snickerdoodle', 0.6056694984436035),\n",
       " ('turtle_cheesecake', 0.6054642200469971),\n",
       " ('yogurt_parfait', 0.6052801609039307)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(recipe.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove words that are not in the Google News model\n",
    "# TODO: Also remove stopwords - not necessary to average e.g. \"in\"\n",
    "filtered_x = []\n",
    "filtered_y_ = []\n",
    "for index, recipe in enumerate(X):\n",
    "    recipe_words = recipe.split()\n",
    "    recipe_words = list(filter(lambda x: x in model.vocab, recipe_words))\n",
    "    if len(recipe_words) > 0: #don't add empty lists - the LinReg can't fit NaNs\n",
    "        filtered_x.append(recipe_words)\n",
    "        filtered_y_.append(y_[index]) # to keep y_ same index\n",
    "    else:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chicken', 'Kale', 'in', 'Parmesan', 'Cream', 'Sauce']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_x[571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_vectorizer(sent, model):\n",
    "    '''\n",
    "    Just averages the vector for each word in a list\n",
    "    '''\n",
    "    sent_vec = np.zeros(300)\n",
    "    numw=0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            sent_vec = np.add(sent_vec, model[w])\n",
    "            numw+=1\n",
    "        except:\n",
    "            pass\n",
    "    return sent_vec / float(numw)\n",
    "\n",
    "def input_preprocessor(dish, model=model):\n",
    "    '''\n",
    "    filter for words that are in the model\n",
    "    '''\n",
    "    \n",
    "    dish = dish.split()\n",
    "    \n",
    "    ngrammed = []\n",
    "    ngrammed.extend(dish)\n",
    "    for index, word in enumerate(dish):\n",
    "        if index+1 < len(dish):\n",
    "            ngrammed.append(word + \"_\" + dish[index+1])\n",
    "    # duplicate words won't help us\n",
    "    ngrammed = set(ngrammed)\n",
    "    print(ngrammed)\n",
    "    # only include words that have a cosine similarity with the word \"food\" above a certain threshold\n",
    "    # better idea: weight them against cosine similarity to food!\n",
    "    foodywords = filter(lambda x: x in model.vocab and model.similarity(x, \"food\") > 0.16, ngrammed)\n",
    "    # if no words are that close to food, pick the top two\n",
    "    if len(foodywords) == 0:\n",
    "        sims = []\n",
    "        for x in ngrammed:\n",
    "            if x in model.vocab:\n",
    "                sims.append((x, model.similarity(x, \"food\")))\n",
    "        foodywords = [word for word, similarity in sorted(sims, key=itemgetter(1))[:2]]\n",
    "    foodywords = set(foodywords)\n",
    "    print(foodywords)\n",
    "#     for term in dish:\n",
    "#         print(term)\n",
    "#         print(model.similarity(term, \"food\"))\n",
    "#     return dish\n",
    "    return sent_vectorizer(foodywords, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can you train a model to know what kinds of words are foods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ground_truth_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b542dd44dd4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ground_truth_X' is not defined"
     ]
    }
   ],
   "source": [
    "input_preprocessor(ground_truth_X[201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_x = []\n",
    "for index, i in enumerate(filtered_x):\n",
    "    sent_x.append(sent_vectorizer(i, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.similarity(\"saucepan\", \"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# embedded_x = np.zeros((len(filtered_x), 300))\n",
    "# for index, i in enumerate(filtered_x):\n",
    "#     embedded_x[index] = model[filtered_x[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linreg = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg.fit(X=sent_x, y=filtered_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dish = input_preprocessor(\"outback steakhouse bloomin onion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linreg.predict([dish])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_X = pkl.load(open(\"tf_data/test_data_names.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_y_ = pkl.load(open(\"tf_data/test_labels_num.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dish in ground_truth_X[:5]:\n",
    "    vdish = input_preprocessor(dish)\n",
    "    print(dish, model.similar_by_vector(vdish, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for index, i in enumerate(ground_truth_X):\n",
    "    predictions.append(linreg.predict([input_preprocessor(i)])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(ground_truth_y_,predictions)\n",
    "slope = float(slope)\n",
    "\n",
    "line = slope*ground_truth_y_+intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_value**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "# Create a trace\n",
    "trace1 = go.Scatter(\n",
    "    x = ground_truth_y_,\n",
    "    y = predictions,\n",
    "    #name=\"Ground truth cholesterol vs \\nlinear regression prediction\",\n",
    "    mode = 'markers'\n",
    ")\n",
    "\n",
    "# trace2 = go.Scatter(\n",
    "#                   x=ground_truth_y_,\n",
    "#                   y=line,\n",
    "#                   mode='lines',\n",
    "#                   marker=go.Marker(color='rgb(31, 119, 180)'),\n",
    "#                   name='Fit',\n",
    "#                   text=test[\"dish_name\"],\n",
    "#                   )\n",
    "\n",
    "annotation = go.Annotation(\n",
    "                  x=800,\n",
    "                  y=180,\n",
    "                  text='$R^2 = 0.11$',\n",
    "                  showarrow=False,\n",
    "                  font=go.Font(size=16)\n",
    "                  )\n",
    "\n",
    "layout = go.Layout(\n",
    "                title='Ground truth cholesterol vs linear model prediction',\n",
    "                plot_bgcolor='rgb(229, 229, 229)',\n",
    "                xaxis=go.XAxis(zerolinecolor='rgb(255,255,255)', gridcolor='rgb(255,255,255)', title= 'Ground truth cholesterol'),\n",
    "                yaxis=go.YAxis(zerolinecolor='rgb(255,255,255)', gridcolor='rgb(255,255,255)', title= 'Predicted cholesterol'),\n",
    "                annotations=[annotation],\n",
    "                showlegend = False,\n",
    "    )\n",
    "\n",
    "data = [trace1]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "py.iplot(fig, filename='Ground truth cholesterol vs linear model prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
