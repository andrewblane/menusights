{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "import re\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Recipe(Base):\n",
    "    __tablename__ = 'recipes'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    url = Column(String, unique=True)\n",
    "    calories = Column(Integer)\n",
    "    fat = Column(Float)\n",
    "    carbs = Column(Float)\n",
    "    protein = Column(Float)\n",
    "    cholesterol = Column(Float)\n",
    "    sodium = Column(Float)\n",
    "    servings = Column(Integer)\n",
    "    #ingredients = \n",
    "    #__table_args__ = {'extend_existing': True}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Recipe(name='%s', url='%s')>\" % (\n",
    "            self.name, self.url)\n",
    "    \n",
    "class Ingredient(Base):\n",
    "    __tablename__ = 'ingredients'\n",
    "    id = Column(Integer, primary_key = True)\n",
    "    ingredient = Column(String, nullable = False)\n",
    "    recipe_id = Column(Integer, ForeignKey('recipes.id'))\n",
    "    \n",
    "    recipe = relationship(Recipe, back_populates = 'ingredients')\n",
    "    #__table_args__ = {'extend_existing': True}\n",
    "    def __repr__(self):\n",
    "        return \"<Ingredient(ingredient='%s')>\" % self.ingredient\n",
    "\n",
    "Recipe.ingredients = relationship(\"Ingredient\", order_by=Ingredient.id, back_populates=\"recipe\")\n",
    "\n",
    "class Restaurant(Base):\n",
    "    __tablename__ = 'restaurants'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    url = Column(String)\n",
    "    zomatoID = Column(Integer, unique=True)\n",
    "    costfortwo = Column(Float)\n",
    "    featured_image = Column(String)\n",
    "    photos = Column(String)\n",
    "    menu_url = Column(String)\n",
    "    price_range = Column(Integer)\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    address = Column(String)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Restaurant(name='%s', url='%s')>\" % (\n",
    "            self.name, self.url)\n",
    "\n",
    "class MenuItem(Base):\n",
    "    __tablename__ = 'menuitems'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    menuitem = Column(String, nullable = False)\n",
    "    description = Column(String)\n",
    "    restaurant_id = Column(Integer, ForeignKey('restaurants.id'))\n",
    "    price = Column(String)\n",
    "    \n",
    "    restaurant = relationship(Restaurant, back_populates = 'menuitems')\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    def __repr__(self):\n",
    "        return \"<MenuItem(item='%s', desc='%s')>\" % (\n",
    "            self.menuitem, self.description)\n",
    "    \n",
    "Restaurant.menuitems = relationship(\"MenuItem\", order_by=MenuItem.id, back_populates=\"restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measurements = (\"femtogram\", \"gigagram\", \"gram\", \"hectogram\", \"kilogram\", \\\n",
    "                \"long\", \"ton\", \"mcg\", \"megagram\", \"metric\", \"ton\", \"metric\"\\\n",
    "                \"tonne\", \"microgram\", \"milligram\",\"nanogram\", \"ounce\", \\\n",
    "                \"lb\", \"oz\", \"each\", \"pound\", \"short\", \"Gram\", \"Ounce\", \"Pint\", \"Quart\",\\\n",
    "                \"Tablespoon\", \"Teaspoon\", \"Tablespoons\", \"Teaspoons\", \"Cups\", \"cup\",\"Fluid Ounce\", \"fl oz\", \"Gallon\", \"Ounce\", \\\n",
    "                \"Pint\", \"Quart\", \"Tablespoon\", \"Teaspoon\", \"liter\", \"litre\", \"L\", \"ml\", \"fluid ounces\", \"can\", \"cans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_measurement_words(i):\n",
    "    ingredient_line = i\n",
    "    for item in measurements:\n",
    "        meas = str(\" \" + item.lower() + \" \")\n",
    "        a = re.search(meas, ingredient_line)\n",
    "        if a > 0:\n",
    "            unit = ingredient_line[a.start():a.end()].strip()\n",
    "            quantity = ingredient_line[:a.start()].strip()\n",
    "            ingredient = ingredient_line[a.end():].strip()\n",
    "            break\n",
    "        else:\n",
    "            ilist = i.split(\" \")\n",
    "            quantity = ilist[0]\n",
    "            unit = \"each\"\n",
    "            ingredient = \" \".join(ilist[1:])\n",
    "    newitem = {\"ingredient\": ingredient,\n",
    "    \"unit\": unit,\n",
    "    \"quantity\":  quantity}\n",
    "    try:\n",
    "        fracsplit = ([float(k) for k in newitem[\"quantity\"].split(\"/\")])\n",
    "        if len(fracsplit) >= 2:\n",
    "            newitem[\"quantity\"] = fracsplit[0] / fracsplit[1]\n",
    "    except:\n",
    "        None\n",
    "    #print(newitem)\n",
    "    return newitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_up_ingredient(ingredient_line):\n",
    "    ingredient_line = re.sub(\"\\[u\\'\", \"\", ingredient_line)\n",
    "    ingredient_line = re.sub(\"\\']\", \"\", ingredient_line)\n",
    "    return find_measurement_words(ingredient_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://andylane@localhost/restaurants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/sqlalchemy/dialects/__init__.py:34: SADeprecationWarning: The SQLAlchemy PostgreSQL dialect has been renamed from 'postgres' to 'postgresql'. The new URL format is postgresql[+driver]://<user>:<pass>@<host>/<dbname>\n",
      "  module = __import__('sqlalchemy.dialects.%s' % (dialect, )).dialects\n"
     ]
    }
   ],
   "source": [
    "dbname = 'restaurants'\n",
    "username = 'andylane'\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = None\n",
    "con = psycopg2.connect(database = \"restaurants\", user = \"andylane\")\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Get recipe names into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = session.query(Recipe).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles=[]\n",
    "for item in names:\n",
    "    titles.append(item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Get ingredient lists into vector, order matching names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synopses = []\n",
    "for item in names:\n",
    "    synopses.append(\" \".join([clean_up_ingredient(a.ingredient)[\"ingredient\"] for a in item.ingredients]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calories=[]\n",
    "for item in names:\n",
    "    calories.append(item.calories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cholesterol=[]\n",
    "for item in names:\n",
    "    cholesterol.append(item.cholesterol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match menu items to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(title):\n",
    "    stemmed_titles = []\n",
    "    new_title=[]\n",
    "    for word in nltk.word_tokenize(title):\n",
    "        new_title.append(stemmer.stem(word))\n",
    "    stemmed_titles.extend(new_title)\n",
    "    return [i for i in stemmed_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['braised', 'short', 'ribs']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"braised short ribs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_recipes(titles, num_clusters = 380): \n",
    "    def tokenize_and_stem(title):\n",
    "        stemmed_titles = []\n",
    "        new_title=[]\n",
    "        for word in nltk.word_tokenize(title):\n",
    "            new_title.append(stemmer.stem(word))\n",
    "        stemmed_titles.extend(new_title)\n",
    "        return [i for i in stemmed_titles]\n",
    "\n",
    "    def tokenize_only(title):\n",
    "        tokenized_titles = []\n",
    "        new_title=[]\n",
    "        for word in nltk.word_tokenize(title):\n",
    "            new_title.append(word)\n",
    "        tokenized_titles.extend(new_title)\n",
    "        return [i for i in tokenized_titles]\n",
    "\n",
    "    #not super pythonic, no, not at all.\n",
    "    #use extend so it's a big flat list of vocab\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in titles:\n",
    "        totalvocab_stemmed.extend([j for j in tokenize_and_stem(i)])\n",
    "        totalvocab_tokenized.extend([k for k in tokenize_only(i)])\n",
    "\n",
    "    vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "    #define vectorizer parameters\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=200000,\n",
    "                                     min_df=0.01, stop_words='english',\n",
    "                                     use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "    count_vectorizer = CountVectorizer(max_df=0.9, max_features=200000,\n",
    "                                     min_df=0.01, stop_words='english', tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(titles) #fit the vectorizer to synopses\n",
    "\n",
    "    print(tfidf_matrix.shape)\n",
    "\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    #num_clusters = 380\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    #uncomment the below to save your model \n",
    "    #since I've already run my model I am loading from the pickle\n",
    "    #joblib.dump(km,  'doc_cluster.pkl')\n",
    "    #km = joblib.load('doc_cluster.pkl')\n",
    "    recipes = {'title': titles, 'synopsis': synopses,'calories': calories,'cholesterol': cholesterol, 'cluster': clusters}\n",
    "    frame = pd.DataFrame(recipes, index = [clusters] , columns = ['title', 'cluster', 'calories', 'cholesterol'])\n",
    "    frame['cluster'].value_counts()\n",
    "    grouped = frame['cholesterol'].groupby(frame['cluster']) #groupby cluster for aggregation purposes\n",
    "    grouped.mean() #average rank (1 to 100) per cluster\n",
    "    return {\"cluster_designations\": frame, \"vectorizer\": tfidf_vectorizer, \"model\": km}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign description to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_ingredients(synopses, num_clusters = 100):\n",
    "    '''\n",
    "    Input: a list containing space-separated ingredients for a set of recipes.\n",
    "    Output: a scikit-learn k-means model object \n",
    "    '''\n",
    "    \n",
    "    # here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "    def tokenize_and_stem(text):\n",
    "        # first tokenize by sentence, then by word to ensure that punctuation is caught as its own token\n",
    "        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        filtered_tokens = []\n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "        stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        return stems\n",
    "\n",
    "\n",
    "    def tokenize_only(text):\n",
    "        # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "        tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        filtered_tokens = []\n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "        return filtered_tokens\n",
    "\n",
    "    #not super pythonic, no, not at all.\n",
    "    #use extend so it's a big flat list of vocab\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in synopses:\n",
    "        allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "\n",
    "        allwords_tokenized = tokenize_only(i)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "    vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "    print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "    print(vocab_frame.head())\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    #define vectorizer parameters\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                     min_df=0.2, stop_words='english',\n",
    "                                     use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "\n",
    "    print(tfidf_matrix.shape)\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    #num_clusters = 100\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    #uncomment the below to save your model \n",
    "    #since I've already run my model I am loading from the pickle\n",
    "    #joblib.dump(km,  'doc_cluster.pkl')\n",
    "    #km = joblib.load('doc_cluster.pkl')\n",
    "    recipes = {'title': titles, 'synopsis': synopses,'calories': calories, 'cholesterol':cholesterol, 'cluster': clusters}\n",
    "    frame = pd.DataFrame(recipes, index = [clusters] , columns = ['title', 'cluster', 'calories', 'cholesterol'])\n",
    "    frame['cluster'].value_counts()\n",
    "    grouped = frame['cholesterol'].groupby(frame['cluster']) #groupby cluster for aggregation purposes\n",
    "    grouped.mean() #average rank (1 to 100) per cluster\n",
    "    return {\"cluster_designations\": frame, \"vectorizer\": tfidf_vectorizer, \"model\": km}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3941, 96)\n"
     ]
    }
   ],
   "source": [
    "recipe_clusters = cluster_recipes(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 117798 items in vocab_frame\n",
      "                  words\n",
      "milk               milk\n",
      "white             white\n",
      "vinegar         vinegar\n",
      "all-purpos  all-purpose\n",
      "flour             flour\n",
      "(3941, 36)\n"
     ]
    }
   ],
   "source": [
    "ingredient_clusters = cluster_ingredients(synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = recipe_clusters['vectorizer'].transform([\"salad\"])\n",
    "prediction = recipe_clusters['model'].predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([272], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicts the cluster that a list of ingredients is in.\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = recipe_clusters[\"cluster_designations\"].ix[prediction[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.83333333333333"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(recipe_clusters[\"cluster_designations\"].ix[prediction[0]][\"cholesterol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ingredient_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_menu_item(menu_item, menu_item_description, recipe_clusters, ingredient_clusters):\n",
    "    menu_item = \" \".join(tokenize_and_stem(menu_item))\n",
    "    vectorized_menu_item = recipe_clusters[\"vectorizer\"].transform([menu_item]) #can later pass a list of menu_items\n",
    "    recipe_cluster = recipe_clusters[\"model\"].predict(vectorized_menu_item)\n",
    "    print(recipe_cluster)\n",
    "    description = \" \".join(tokenize_and_stem(menu_item_description))\n",
    "    #look up ingredient clusters list; return found ingredient clusters matching recipes\n",
    "    # assign menu item description to an ingredient cluster\n",
    "    # ask if there's a match \n",
    "    # for any \n",
    "    for item in recipe_cluster:\n",
    "        print(item[1])\n",
    "        recipeid = session.query(Recipe).filter(Recipe.name == item[1]).all()[0]\n",
    "        \n",
    "        #print session.query(Ingredient).filteritem[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return recipe_clusters[\"cluster_designations\"].ix[recipe_cluster]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recipe_clusters[110].transform([menu_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classify_menu_item(\"Chicken pad thai\", \"\", recipe_clusters, ingredient_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "menu_items = session.query(MenuItem).filter(MenuItem.restaurant_id == 82).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[item.menuitem.encode(\"utf-8\") for item in session.query(MenuItem).filter(MenuItem.restaurant_id == 9).all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item for item in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAENCAYAAAAi3P88AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG7IrYefnqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9bbc27550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.boxplot(x=a[\"cholesterol\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [insight]",
   "language": "python",
   "name": "Python [insight]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
