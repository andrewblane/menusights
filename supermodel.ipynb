{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, ForeignKey\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "import re\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Recipe(Base):\n",
    "    __tablename__ = 'recipes'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    url = Column(String, unique=True)\n",
    "    calories = Column(Integer)\n",
    "    fat = Column(Float)\n",
    "    carbs = Column(Float)\n",
    "    protein = Column(Float)\n",
    "    cholesterol = Column(Float)\n",
    "    sodium = Column(Float)\n",
    "    servings = Column(Integer)\n",
    "    #ingredients = \n",
    "    #__table_args__ = {'extend_existing': True}\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Recipe(name='%s', url='%s')>\" % (\n",
    "            self.name, self.url)\n",
    "    \n",
    "class Ingredient(Base):\n",
    "    __tablename__ = 'ingredients'\n",
    "    id = Column(Integer, primary_key = True)\n",
    "    ingredient = Column(String, nullable = False)\n",
    "    recipe_id = Column(Integer, ForeignKey('recipes.id'))\n",
    "    \n",
    "    recipe = relationship(Recipe, back_populates = 'ingredients')\n",
    "    #__table_args__ = {'extend_existing': True}\n",
    "    def __repr__(self):\n",
    "        return \"<Ingredient(ingredient='%s')>\" % self.ingredient\n",
    "\n",
    "Recipe.ingredients = relationship(\"Ingredient\", order_by=Ingredient.id, back_populates=\"recipe\")\n",
    "\n",
    "class Restaurant(Base):\n",
    "    __tablename__ = 'restaurants'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String)\n",
    "    url = Column(String)\n",
    "    zomatoID = Column(Integer, unique=True)\n",
    "    costfortwo = Column(Float)\n",
    "    featured_image = Column(String)\n",
    "    photos = Column(String)\n",
    "    menu_url = Column(String)\n",
    "    price_range = Column(Integer)\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    address = Column(String)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Restaurant(name='%s', url='%s')>\" % (\n",
    "            self.name, self.url)\n",
    "\n",
    "class MenuItem(Base):\n",
    "    __tablename__ = 'menuitems'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    menuitem = Column(String, nullable = False)\n",
    "    description = Column(String)\n",
    "    restaurant_id = Column(Integer, ForeignKey('restaurants.id'))\n",
    "    price = Column(String)\n",
    "    \n",
    "    restaurant = relationship(Restaurant, back_populates = 'menuitems')\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    def __repr__(self):\n",
    "        return \"<MenuItem(item='%s', desc='%s')>\" % (\n",
    "            self.menuitem, self.description)\n",
    "    \n",
    "Restaurant.menuitems = relationship(\"MenuItem\", order_by=MenuItem.id, back_populates=\"restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measurements = (\"femtogram\", \"gigagram\", \"gram\", \"hectogram\", \"kilogram\", \\\n",
    "                \"long\", \"ton\", \"mcg\", \"megagram\", \"metric\", \"ton\", \"metric\"\\\n",
    "                \"tonne\", \"microgram\", \"milligram\",\"nanogram\", \"ounce\", \\\n",
    "                \"lb\", \"oz\", \"each\", \"pound\", \"short\", \"Gram\", \"Ounce\", \"Pint\", \"Quart\",\\\n",
    "                \"Tablespoon\", \"Teaspoon\", \"Tablespoons\", \"Teaspoons\", \"Cups\", \"cup\",\"Fluid Ounce\", \"fl oz\", \"Gallon\", \"Ounce\", \\\n",
    "                \"Pint\", \"Quart\", \"Tablespoon\", \"Teaspoon\", \"liter\", \"litre\", \"L\", \"ml\", \"fluid ounces\", \"can\", \"cans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_measurement_words(i):\n",
    "    ingredient_line = i\n",
    "    for item in measurements:\n",
    "        meas = str(\" \" + item.lower() + \" \")\n",
    "        a = re.search(meas, ingredient_line)\n",
    "        if a > 0:\n",
    "            unit = ingredient_line[a.start():a.end()].strip()\n",
    "            quantity = ingredient_line[:a.start()].strip()\n",
    "            ingredient = ingredient_line[a.end():].strip()\n",
    "            break\n",
    "        else:\n",
    "            ilist = i.split(\" \")\n",
    "            quantity = ilist[0]\n",
    "            unit = \"each\"\n",
    "            ingredient = \" \".join(ilist[1:])\n",
    "    newitem = {\"ingredient\": ingredient,\n",
    "    \"unit\": unit,\n",
    "    \"quantity\":  quantity}\n",
    "    try:\n",
    "        fracsplit = ([float(k) for k in newitem[\"quantity\"].split(\"/\")])\n",
    "        if len(fracsplit) >= 2:\n",
    "            newitem[\"quantity\"] = fracsplit[0] / fracsplit[1]\n",
    "    except:\n",
    "        None\n",
    "    #print(newitem)\n",
    "    return newitem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_up_ingredient(ingredient_line):\n",
    "    ingredient_line = re.sub(\"\\[u\\'\", \"\", ingredient_line)\n",
    "    ingredient_line = re.sub(\"\\']\", \"\", ingredient_line)\n",
    "    return find_measurement_words(ingredient_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgres://andylane@localhost/restaurants\n"
     ]
    }
   ],
   "source": [
    "dbname = 'restaurants'\n",
    "username = 'andylane'\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "print(engine.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "con = None\n",
    "con = psycopg2.connect(database = \"restaurants\", user = \"andylane\")\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Get recipe names into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = session.query(Recipe).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles=[]\n",
    "for item in names:\n",
    "    titles.append(item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Get ingredient lists into vector, order matching names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synopses = []\n",
    "for item in names:\n",
    "    synopses.append(\" \".join([clean_up_ingredient(a.ingredient)[\"ingredient\"] for a in item.ingredients]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calories=[]\n",
    "for item in names:\n",
    "    calories.append(item.calories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match menu items to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(title):\n",
    "    stemmed_titles = []\n",
    "    new_title=[]\n",
    "    for word in nltk.word_tokenize(title):\n",
    "        new_title.append(stemmer.stem(word))\n",
    "    stemmed_titles.extend(new_title)\n",
    "    return [i for i in stemmed_titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['braised', 'short', 'ribs']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"braised short ribs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_recipes(titles, num_clusters = 380): \n",
    "    def tokenize_and_stem(title):\n",
    "        stemmed_titles = []\n",
    "        new_title=[]\n",
    "        for word in nltk.word_tokenize(title):\n",
    "            new_title.append(stemmer.stem(word))\n",
    "        stemmed_titles.extend(new_title)\n",
    "        return [i for i in stemmed_titles]\n",
    "\n",
    "    def tokenize_only(title):\n",
    "        tokenized_titles = []\n",
    "        new_title=[]\n",
    "        for word in nltk.word_tokenize(title):\n",
    "            new_title.append(word)\n",
    "        tokenized_titles.extend(new_title)\n",
    "        return [i for i in tokenized_titles]\n",
    "\n",
    "    #not super pythonic, no, not at all.\n",
    "    #use extend so it's a big flat list of vocab\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in titles:\n",
    "        totalvocab_stemmed.extend([j for j in tokenize_and_stem(i)])\n",
    "        totalvocab_tokenized.extend([k for k in tokenize_only(i)])\n",
    "\n",
    "    vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "    #define vectorizer parameters\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.9, max_features=200000,\n",
    "                                     min_df=0.01, stop_words='english',\n",
    "                                     use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "    count_vectorizer = CountVectorizer(max_df=0.9, max_features=200000,\n",
    "                                     min_df=0.01, stop_words='english', tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(titles) #fit the vectorizer to synopses\n",
    "\n",
    "    print(tfidf_matrix.shape)\n",
    "\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    #num_clusters = 380\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    #uncomment the below to save your model \n",
    "    #since I've already run my model I am loading from the pickle\n",
    "    #joblib.dump(km,  'doc_cluster.pkl')\n",
    "    #km = joblib.load('doc_cluster.pkl')\n",
    "    recipes = {'title': titles, 'synopsis': synopses,'calories': calories, 'cluster': clusters}\n",
    "    frame = pd.DataFrame(recipes, index = [clusters] , columns = ['title', 'cluster', 'calories'])\n",
    "    frame['cluster'].value_counts()\n",
    "    grouped = frame['calories'].groupby(frame['cluster']) #groupby cluster for aggregation purposes\n",
    "    grouped.mean() #average rank (1 to 100) per cluster\n",
    "    return {\"cluster_designations\": frame, \"vectorizer\": tfidf_vectorizer, \"model\": km}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign description to a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_ingredients(synopses, num_clusters = 100):\n",
    "    '''\n",
    "    Input: a list containing space-separated ingredients for a set of recipes.\n",
    "    Output: a scikit-learn k-means model object \n",
    "    '''\n",
    "    \n",
    "    # here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "    def tokenize_and_stem(text):\n",
    "        # first tokenize by sentence, then by word to ensure that punctuation is caught as its own token\n",
    "        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        filtered_tokens = []\n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "        stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "        return stems\n",
    "\n",
    "\n",
    "    def tokenize_only(text):\n",
    "        # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "        tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "        filtered_tokens = []\n",
    "        # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "        return filtered_tokens\n",
    "\n",
    "    #not super pythonic, no, not at all.\n",
    "    #use extend so it's a big flat list of vocab\n",
    "    totalvocab_stemmed = []\n",
    "    totalvocab_tokenized = []\n",
    "    for i in synopses:\n",
    "        allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "        totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "\n",
    "        allwords_tokenized = tokenize_only(i)\n",
    "        totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "    vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "    print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "    print(vocab_frame.head())\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    #define vectorizer parameters\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                     min_df=0.2, stop_words='english',\n",
    "                                     use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n",
    "\n",
    "    print(tfidf_matrix.shape)\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "    #num_clusters = 100\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    #uncomment the below to save your model \n",
    "    #since I've already run my model I am loading from the pickle\n",
    "    #joblib.dump(km,  'doc_cluster.pkl')\n",
    "    #km = joblib.load('doc_cluster.pkl')\n",
    "    recipes = {'title': titles, 'synopsis': synopses,'calories': calories, 'cluster': clusters}\n",
    "    frame = pd.DataFrame(recipes, index = [clusters] , columns = ['title', 'cluster', 'calories'])\n",
    "    frame['cluster'].value_counts()\n",
    "    grouped = frame['calories'].groupby(frame['cluster']) #groupby cluster for aggregation purposes\n",
    "    grouped.mean() #average rank (1 to 100) per cluster\n",
    "    return {\"cluster_designations\": frame, \"vectorizer\": tfidf_vectorizer, \"model\": km}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3941, 96)\n"
     ]
    }
   ],
   "source": [
    "recipe_clusters = cluster_recipes(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 117798 items in vocab_frame\n",
      "                  words\n",
      "milk               milk\n",
      "white             white\n",
      "vinegar         vinegar\n",
      "all-purpos  all-purpose\n",
      "flour             flour\n",
      "(3941, 36)\n"
     ]
    }
   ],
   "source": [
    "ingredient_clusters = cluster_ingredients(synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = recipe_clusters['vectorizer'].transform([\"Soup\"])\n",
    "prediction = recipe_clusters['model'].predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([306], dtype=int32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicts the cluster that a list of ingredients is in.\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "306",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-1a8effe58290>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecipe_clusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cluster_designations\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m306\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1016\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no slices here, handle elsewhere'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, copy, drop_level)\u001b[0m\n\u001b[0;32m   1747\u001b[0m                                                       drop_level=drop_level)\n\u001b[0;32m   1748\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andylane/anaconda2/envs/insight/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1945\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1946\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1947\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3955)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine._get_loc_duplicates (pandas/index.c:4385)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.Int64Engine._maybe_get_bool_indexer (pandas/index.c:8079)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 306"
     ]
    }
   ],
   "source": [
    "recipe_clusters[\"cluster_designations\"].ix[306]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_menu_item(menu_item, menu_item_description, recipe_clusters, ingredient_clusters):\n",
    "    menu_item = \" \".join(tokenize_and_stem(menu_item))\n",
    "    vectorized_menu_item = recipe_clusters[\"vectorizer\"].transform([menu_item]) #can later pass a list of menu_items\n",
    "    recipe_cluster = recipe_clusters[\"model\"].predict(vectorized_menu_item)\n",
    "    print(recipe_cluster)\n",
    "    description = \" \".join(tokenize_and_stem(description))\n",
    "    #look up ingredient clusters list; return found ingredient clusters matching recipes\n",
    "    # assign menu item description to an ingredient cluster\n",
    "    # ask if there's a match \n",
    "    # for any \n",
    "    return recipe_clusters[\"cluster_designations\"].ix[recipe_cluster]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112                                   Garlic Pork Kabobs\n",
       "112                        Skillet Garlic Chicken Dinner\n",
       "112                         Roasted Garlic Chicken Penne\n",
       "112                                 Lemon Garlic Chicken\n",
       "112                        Garlic Caesar Chicken Tenders\n",
       "112                        Garlic Cheese Chicken Rollups\n",
       "112    Crispy Chicken Croquettes with Garlic Butter S...\n",
       "112    Steamed Fresh Green Beans with Garlic Dill Hol...\n",
       "112                                   Chicken and Garlic\n",
       "112                                  Lime Garlic Chicken\n",
       "112    Garlic Chicken Sausage and Summer Vegetable Saute\n",
       "112                                 Garlic Chicken Pizza\n",
       "112                          Garlic Chicken and Zucchini\n",
       "112                  Chard Stalks and Garlic Scape Pasta\n",
       "112                     Chicken with 40 Cloves of Garlic\n",
       "112                    Chicken with 20 Cloves of Garlic \n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_menu_item(\"garlic pizza\", \"pizza smothered with garlic cloves and tomatoes\", recipe_clusters, ingredient_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "menu_items = session.query(MenuItem).filter(MenuItem.restaurant_id == 82).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'almond milk - calabrian chili'),\n",
       " (1,\n",
       "  'baby lettuces - pecan - pomegranate - preserved meyer lemon - fennel - sheep milk cheddar - apple cider vinaigrette'),\n",
       " (2,\n",
       "  'roasted cauliflower - arugula - spiced pepita - scallion - pecorino - maple - sherry vinaigrette'),\n",
       " (3,\n",
       "  \"apple - celery root slaw - dill - avocado - leadbetter's english muffin - greens / fries\"),\n",
       " (4,\n",
       "  \"fontina - mizuna- pickled red onion - sweet chili sauce - aioli - leadbetter's english muffin - greens / fries\"),\n",
       " (5,\n",
       "  'curried carrot slaw - cucumber & mint raita - torpedo roll - greens / fries'),\n",
       " (6,\n",
       "  'housemade chickpea and mushroom patty - arugula - pickled jalape\\xc3\\xb1os - chipotle aioli - gruy\\xc3\\xa8re - acme roll - greens / fries'),\n",
       " (7,\n",
       "  'arugula - pickled red onions - bread & butter pickles - acme roll - greens / fries'),\n",
       " (8, 'bacon - chili flake - spinach - grana padano'),\n",
       " (9, ''),\n",
       " (10, ''),\n",
       " (11, 'seven stills vodka, aqua di cedro, lemon verbana'),\n",
       " (12, 'blade gin, elderflower liqueur, grapefruit juice, sparkling wine'),\n",
       " (13, 'skipper dark rum - rhum orange liquor - pomegranate, lime, nutmeg rum'),\n",
       " (14, 'santa theresa rum, sage, honey, lemon thyme'),\n",
       " (15, 'tequila blanco, habenero syrup, lime, smoked salt rim'),\n",
       " (16,\n",
       "  'high west double rye, g\\xc3\\xa9n\\xc3\\xa9py, sutton cellars vermouth, angostura + orange bitters, lemon twist'),\n",
       " (17, 'redemption rye, lemon, maple, absinthe'),\n",
       " (18, 'henry mckenna bourbon, gran classico, cynar, barrel aged bitters'),\n",
       " (19, 'evan williams bourbon, muddled lemon, mint'),\n",
       " (20, ''),\n",
       " (21, 'fresh horseradish, champagne mignonette, marin miyagi /beau soleil'),\n",
       " (22, 'yogurt, honey, chili powder, gomasio salt'),\n",
       " (23, 'chives, spiced cashews, chili oil'),\n",
       " (24,\n",
       "  'duck prosciutto, bresaola, speck, napoli, duck liver mousse, cornichons & caper berries, apple mostarda, grilled levain toast'),\n",
       " (25,\n",
       "  'lamborn bloomers (pasteurized goat), little lamb (pasteurized sheep), bent river (pasteurized cow) with dates & cranberry walnut levain'),\n",
       " (26,\n",
       "  'baby kale, chicories, smoked paprika, ricotta salata, pomegranate, lime & maple vinaigrette'),\n",
       " (27,\n",
       "  'arugula, little gem, fennel, mint, spiced pecans, cloth bound cheddar, apple cider vinaigrette'),\n",
       " (28, 'warm garbanzo bean, tomato, mint'),\n",
       " (29,\n",
       "  'fenugreek spice, thai bird chili, cilantro, mizuna, hearts of palm salad'),\n",
       " (30, 'black & green cardamom, fennel, yogurt, sorrel pur\\xc3\\xa9e'),\n",
       " (31,\n",
       "  'butternut squash - chestnut - cr\\xc3\\xa9me fra\\xc3\\xaeche - truffle butter - oregano - green garlic - pecorino'),\n",
       " (32,\n",
       "  'roasted kohlrabi - fennel - thai eggplant - basil - cauliflower - basmati rice - yellow curry'),\n",
       " (33,\n",
       "  'farro - bacon lardon - brussels sprouts - chard - rosemary & sherry jus'),\n",
       " (34,\n",
       "  'celery root & pear pure\\xc3\\xa9 - roasted root vegetables - swiss chard- cranberry mostarda - spiced almonds'),\n",
       " (35,\n",
       "  \"arugula - housemade bread n' butter pickles - pickled onions - acme roll - french fries\"),\n",
       " (36,\n",
       "  'roasted sunchoke - carrot - shishito pepper - butterball potato - pickled lime & nettle puree'),\n",
       " (37, 'caper - raisin - chilli flake - grana padano'),\n",
       " (38, 'garlic - shallot - preserved meyer lemon - chili flake'),\n",
       " (39, 'with aioli'),\n",
       " (40, 'milk or yogurt - pomegranate - banana'),\n",
       " (41,\n",
       "  \"apple - banana - pomegranate - candied walnuts mead & mead's maple syrup - whipped cream\"),\n",
       " (42,\n",
       "  \"two eggs - neuske's bacon or house pork sausage - crispy potatoes - grilled levain\"),\n",
       " (43,\n",
       "  'fried egg - arugula - gruyere - dill aioli - crispy potatoes - english muffin'),\n",
       " (44,\n",
       "  'beef brisket - crispy potatoes - roasted beets - arugula horseradish cr\\xc3\\xa9me fraiche - poached eggs - grilled levain'),\n",
       " (45,\n",
       "  'two eggs - spiced black beans - cilantro - avocado - hen of the woods mushrooms - sour cream - queso fresco - ranchero sauce - corn tortillas'),\n",
       " (46,\n",
       "  'two poached eggs - hollandaise sauce - spinach - fried lemon - fines herbes - buttermilk biscuit - crispy potatoes'),\n",
       " (47,\n",
       "  'flat iron steak - scrambled eggs - biscuit & gravy - crispy potatoes - arugula'),\n",
       " (48,\n",
       "  'braised oxtail - creamy polenta - fried eggs - grana padano - piquillo & poblano pepper salsa'),\n",
       " (49,\n",
       "  'baby lettuces - toasted pecan - pomegranate - preserved meyer lemon - sheep milk cheddar - apple cider vinaigrette'),\n",
       " (50,\n",
       "  'two eggs - kohlrabi - carrot - onion - fontina cheese - roasted tomatillo salsa - side of mixed greens'),\n",
       " (51,\n",
       "  'arugula - pickled red onions - housemade pickles - acme roll - french fries / greens'),\n",
       " (52, ''),\n",
       " (53, ''),\n",
       " (54, ''),\n",
       " (55, ''),\n",
       " (56, ''),\n",
       " (57, ''),\n",
       " (58, 'with butter & homemade jam'),\n",
       " (59, 'with butter & housemade jam')]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(index, item.description.encode('utf-8')) for index, item in enumerate(menu_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [insight]",
   "language": "python",
   "name": "Python [insight]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
