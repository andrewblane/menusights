{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing a naive neural net using word embeddings to a linear model for prediction of cholesterol content of foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andylane/Envs/tf/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from operator import itemgetter\n",
    "import _pickle as pkl\n",
    "import random\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import *\n",
    "# from sklearn.linear_model import *\n",
    "import sklearn\n",
    "from sklearn.metrics import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "from tensorflow.contrib import learn\n",
    "import gensim\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "These were previously collected from a collection of recipes with cholesterol information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained Google News word vector embeddings\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#model.save(\"saved_word2vec.mdl\")\n",
    "#del model\n",
    "model = gensim.models.KeyedVectors.load('saved_word2vec.mdl', mmap='r')\n",
    "\n",
    "# Load the traning/test data\n",
    "words = pkl.load(open(\"tf_data/training_data_names.pkl\", \"rb\")) # this is a stemmed/tokenized version\n",
    "X = pkl.load(open(\"tf_data/training_data_names_unstemmed.pkl\", \"rb\")) # Recipe names\n",
    "y = pkl.load(open(\"tf_data/training_labels_num.pkl\", \"rb\")) # Corresponding cholesterol content for each recipe\n",
    "\n",
    "# Slice out 20% for test data. This isn't random. OK for development, but need to go back and check later.\n",
    "X_test = X[3152:]\n",
    "y_test = y[3152:]\n",
    "\n",
    "# The rest is the training data\n",
    "X = X[:3152]\n",
    "y = y[:3152]\n",
    "\n",
    "# We also have some alternative test data from chain restaurant menus, but there are issues\n",
    "# with testing the model on this. Explain in blog.\n",
    "ground_truth_X = pkl.load(open(\"tf_data/test_data_names.pkl\", \"rb\"))\n",
    "ground_truth_y = pkl.load(open(\"tf_data/test_labels_num.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we're carrying out the prediction as a regression\n",
    "# rather than a classification. To start with, an r2 of \n",
    "# predicted vs true test cholesterol is used as a metric; the MSE\n",
    "# is also used.\n",
    "import scipy\n",
    "def rsquared(x, y):\n",
    "    \"\"\" Return R^2 where x and y are array-like.\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    return r_value**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campfire Banana Splits\n",
      "campfir banana split\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cinnamon_rolls', 0.655969500541687),\n",
       " ('oatmeal_cookie', 0.6287018656730652),\n",
       " ('peach_pie', 0.6216853857040405),\n",
       " ('banana_walnut', 0.6158987879753113),\n",
       " ('baked_beans_peaches', 0.6140761375427246),\n",
       " ('waffle_cone', 0.6071436405181885),\n",
       " ('fruit_cobbler', 0.6069991588592529),\n",
       " ('snickerdoodle', 0.6056694984436035),\n",
       " ('turtle_cheesecake', 0.6054642200469971),\n",
       " ('yogurt_parfait', 0.6052801609039307)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notes to self:\n",
    "## There are 300,000 words in the Google News model\n",
    "len(model.vocab)\n",
    "\n",
    "## Get words from given index\n",
    "model.wv.index2word[2898]\n",
    "# Out: 'judges\"\n",
    "\n",
    "## Find the odd word out:\n",
    "model.doesnt_match(X[3].split())\n",
    "\n",
    "## Access recipes:\n",
    "print(X[3])\n",
    "print(words[3])\n",
    "\n",
    "## Get the similarity between two phrases. This is implemented in gensim as\n",
    "## just the cosine similarity of the means of the two vectors\n",
    "model.n_similarity(X[3].split(), X[7].split())\n",
    "model.similarity(\"saucepan\", \"food\")\n",
    "\n",
    "## There's a case for n-gramming with underscores:\n",
    "model.most_similar([\"cinnamon\", \"roll\"])\n",
    "# Out e.g.: coconut_flakes, graham_cracker_crumbs\n",
    "model.most_similar(\"cinnamon_roll\")\n",
    "# Out e.g.: cinnamon_rolls, oatmeal_cookie, peach_pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove words that are not in the Google News model\n",
    "# TODO: Also remove stopwords - not necessary to average e.g. \"in\"\n",
    "def prune_words_not_found_in_model(X, y):\n",
    "    filtered_x = []\n",
    "    filtered_y = []\n",
    "    for index, recipe in enumerate(X):\n",
    "        recipe_words = recipe.split()\n",
    "        recipe_words = list(filter(lambda x: x in model.vocab, recipe_words))\n",
    "        if len(recipe_words) > 0: #don't add empty lists - the LinReg can't fit NaNs\n",
    "            filtered_x.append(recipe_words)\n",
    "            filtered_y.append(y[index]) # to keep y_ same index\n",
    "        else:\n",
    "            None\n",
    "    return filtered_x, filtered_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_preprocessor(dish, model=model, foodfilter=True, maintain_individual_words = True):\n",
    "    '''\n",
    "    filter for words that are in the model and extract bigrams\n",
    "    also optionally only keep words above a cosine similarity\n",
    "    threshold with the word \"food\".\n",
    "    Usage: input_preprocessor(dish). Returns np.array of size model dimensions \n",
    "        (300 dimensions for GoogleNews Word2Vec model)\n",
    "    dish: list of words in recipe title\n",
    "    model: a word2vec model\n",
    "    ''' \n",
    "    # Construct n-grams of pairs of words\n",
    "    ngrammed = []\n",
    "    ngrammed.extend(dish)\n",
    "    for index, word in enumerate(dish):\n",
    "        if index+1 < len(dish):\n",
    "            ngrammed.append(word + \"_\" + dish[index+1])\n",
    "    # Duplicate words in a dish won't help the model\n",
    "    ngrammed = set(ngrammed)\n",
    "    # only include words that have a cosine similarity with the word \"food\" above a certain threshold\n",
    "    # better idea: weight them against cosine similarity to food!\n",
    "    if foodfilter == True:\n",
    "        foodywords = list(filter(lambda x: x in model.vocab and model.similarity(x, \"food\") > 0.16, ngrammed))\n",
    "        # if no words are that close to food, pick the top two words\n",
    "        if len(foodywords) == 0:\n",
    "            sims = []\n",
    "            for x in ngrammed:\n",
    "                if x in model.vocab:\n",
    "                    sims.append((x, model.similarity(x, \"food\")))\n",
    "            foodywords = [word for word, similarity in sorted(sims, key=itemgetter(1))[:2]]\n",
    "        foodywords = set(foodywords)\n",
    "    elif foodfilter == False: # don't filter based on similarity to a food\n",
    "        foodywords = list(filter(lambda x: x in model.vocab, ngrammed))\n",
    "    # Return the average of all the foody-word vectors in the recipe name\n",
    "    if maintain_individual_words == False:\n",
    "        return np.array([model[w] for w in foodywords]).mean(axis=0)\n",
    "    elif maintain_individual_words == True:\n",
    "        # Just get the first 7 words in the dish name\n",
    "        dish_vector = np.array([model[w] for w in dish][:8])\n",
    "        words_in_dish = dish_vector.shape[0]\n",
    "        words_to_pad = 8 - words_in_dish\n",
    "        return np.pad(dish_vector, [(0,words_to_pad),(0,0)], mode=\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the input preprocessor across the filtered list of recipes\n",
    "def prune_and_preprocess(X, y, foodfilter=True):\n",
    "    filtered_x, filtered_y  = prune_words_not_found_in_model(X, y)\n",
    "    filtered_x_vect = []\n",
    "    for index, i in enumerate(filtered_x):\n",
    "        filtered_x_vect.append(input_preprocessor(i, model, foodfilter=foodfilter))\n",
    "    filtered_x_vect = np.array(filtered_x_vect)\n",
    "    filtered_y = np.array([filtered_y]).transpose()\n",
    "    filtered_y = np.array(filtered_y)\n",
    "    return filtered_x_vect, np.transpose(filtered_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_x_vect, filtered_y = prune_and_preprocess(X, y, foodfilter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect, y_test_vect = prune_and_preprocess(X_test, y_test, foodfilter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_x_vect = np.expand_dims(filtered_x_vect, axis=3)\n",
    "X_test_vect  = np.expand_dims(X_test_vect, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 8, 300, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_x_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.data import Dataset, Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posit that cholesterol is a combination of input word vector, some weight, some bias and some error term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input layer\n",
    "def weight_variable(shape):\n",
    "    initial = tf.random_normal(shape, stddev=0.2)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.001, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Build a linear layer\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def conv1d(x, W):\n",
    "    return(tf.nn.conv1d(x, W, stride = 1, padding = 'SAME'))\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "def max_pool_1x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, 1, 1],\n",
    "                          strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]]) #check this index - try3\n",
    "    return tf.nn.relu(conv2d(input, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 4313.7197265625\n",
      "test accuracy: 12605.2685546875\n",
      "step 10000, training accuracy 2232.4365234375\n",
      "test accuracy: 4107.21337890625\n",
      "step 20000, training accuracy 1083.1651611328125\n",
      "test accuracy: 3991.990478515625\n",
      "step 30000, training accuracy 3883.772216796875\n",
      "test accuracy: 3993.88232421875\n",
      "step 40000, training accuracy 5304.74853515625\n",
      "test accuracy: 4045.876953125\n"
     ]
    }
   ],
   "source": [
    "# Try with convolution\n",
    "tf.reset_default_graph()\n",
    "minibatch_size = 32\n",
    "steps_per_epoch = int(filtered_y.shape[1] / minibatch_size)\n",
    "epochs = 1000\n",
    "g=tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    training_data = Dataset.from_tensor_slices((filtered_x_vect, filtered_y.transpose()))\n",
    "    training_data=training_data.batch(minibatch_size).repeat()\n",
    "    iterator = training_data.make_one_shot_iterator()\n",
    "    \n",
    "    steps = epochs*steps_per_epoch\n",
    "    beta = 0.001\n",
    "    batched_x, batched_y = iterator.get_next() \n",
    "    \n",
    "    conv1 = conv_layer(batched_x, shape=[4, 1, 1, 8])\n",
    "    conv1_pool = max_pool_1x1(conv1)\n",
    "\n",
    "    \n",
    "    #     conv1_pool = max_pool_2x2(conv1)\n",
    "\n",
    "#     conv2 = conv_layer(conv1_pool, shape=[2, 2, 8, 12])\n",
    "#     conv2_pool = max_pool_2x2(conv2)\n",
    "\n",
    "    conv2_flat = tf.reshape(conv1_pool, [-1, 8*300*8])\n",
    "    full_1 = tf.nn.relu(full_layer(conv2_flat, 10))\n",
    "\n",
    "    #keep_prob = tf.placeholder(tf.float32)\n",
    "    #full1_drop = tf.nn.dropout(full_1, keep_prob=1)\n",
    "\n",
    "    linear = full_layer(full_1, 1)\n",
    "\n",
    "    regularizer = tf.nn.l2_loss(linear)\n",
    "    mse = tf.losses.mean_squared_error(labels=batched_y, predictions=linear)\n",
    "    mse = tf.reduce_mean(mse + beta * regularizer)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(mse)\n",
    "    accuracy = tf.losses.mean_squared_error(labels=batched_y, predictions=linear)\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(steps):\n",
    "        if i % 10000 == 0:\n",
    "            train_accuracy = sess.run(accuracy)\n",
    "            print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "            test_accuracy = np.mean([sess.run(accuracy, feed_dict={batched_x: X_test_vect, batched_y: y_test_vect.transpose()})])\n",
    "            print(\"test accuracy: {}\".format(test_accuracy))\n",
    "        sess.run(train_step)\n",
    "    test_accuracy = np.mean(\n",
    "        [sess.run(accuracy, feed_dict={batched_x: X_test_vect, batched_y: y_test_vect.transpose()})])\n",
    "    predictions = sess.run(linear, feed_dict={batched_x: X_test_vect, batched_y: y_test_vect.transpose()})\n",
    "print(\"test accuracy: {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4efdd94358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictionsi, truthi = [int(i) for i in predictions.transpose()[0]], [i for i in y_test_vect[0]]\n",
    "plt.plot(truthi, predictionsi, 'r.')\n",
    "plt.ylabel('predictions')\n",
    "plt.xlabel('truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12748859001058499"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsquared(truthi, predictionsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input layer\n",
    "def weight_variable(shape):\n",
    "    initial = tf.random_normal(shape, stddev=0.2)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.001, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Build a linear layer\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def conv1d(x, W):\n",
    "    return(tf.nn.conv1d(x, W, stride = 1, padding = 'SAME'))\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[2]]) #check this index\n",
    "    return tf.nn.relu(conv1d(input, W) + b)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides = [1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "minibatch_size = 32\n",
    "steps_per_epoch = int(filtered_y.shape[1] / minibatch_size)\n",
    "epochs = 1000\n",
    "g=tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    training_data = Dataset.from_tensor_slices((filtered_x_vect, filtered_y.transpose()))\n",
    "    training_data=training_data.batch(minibatch_size).repeat()\n",
    "    iterator = training_data.make_one_shot_iterator()\n",
    "    \n",
    "    steps = epochs*steps_per_epoch\n",
    "    beta = 0.0001\n",
    "    batched_x, batched_y = iterator.get_next() \n",
    "    tanh = tf.nn.tanh(full_layer(batched_x, 10))\n",
    "    relu = tf.nn.relu(full_layer(tanh, 10))\n",
    "    linear = full_layer(relu, 1)\n",
    "\n",
    "    regularizer = tf.nn.l2_loss(linear)\n",
    "    mse = tf.losses.mean_squared_error(labels=batched_y, predictions=linear)\n",
    "    mse = tf.reduce_mean(mse + beta * regularizer)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(mse)\n",
    "    accuracy = tf.losses.mean_squared_error(labels=batched_y, predictions=linear)\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(steps):\n",
    "        if i % 10000 == 0:\n",
    "            train_accuracy = sess.run(accuracy)\n",
    "            print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "            test_accuracy = np.mean(\n",
    "        [sess.run(accuracy, feed_dict={batched_x: X_test_vect, batched_y: y_test_vect.transpose()})])\n",
    "            print(\"test accuracy: {}\".format(test_accuracy))\n",
    "        sess.run(train_step)\n",
    "    test_accuracy = np.mean(\n",
    "        [sess.run(accuracy, feed_dict={batched_x: X_test_vect, batched_y: y_test_vect.transpose()})])\n",
    "    predictions = sess.run(linear, feed_dict={batched_x: X_test_vect, batched_y: y_test_vect.transpose()})\n",
    "print(\"test accuracy: {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10615209824128694"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsquared(truthi, predictionsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_epochs(model, result_, subset=None):\n",
    "    i = 1\n",
    "    if subset != None:\n",
    "        result_[\"history\"] = np.take(result_[\"history\"], range(0, len(result_[\"epoch\"]), subset), axis=1)\n",
    "        result_[\"epoch\"] = result_[\"epoch\"][::subset]\n",
    "    while i < result_[\"history\"].shape[1]:\n",
    "        plt.plot(result_[\"history\"][:,0], result_[\"history\"][:,i], 'r.')\n",
    "        plt.ylabel('y_')\n",
    "        plt.xlabel('y')\n",
    "        plt.show()\n",
    "        print(\"Epoch \" + str(result_[\"epoch\"][i]) + \": R2 = \" + str(rsquared(result_[\"history\"][:,0], result_[\"history\"][:,i])))\n",
    "        print(\"Epoch \" + str(result_[\"epoch\"][i]) + \": MSE = \" + str(mean_squared_error(result_[\"history\"][:,0], result_[\"history\"][:,i])))\n",
    "        i+= 1\n",
    "#     return result[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsquared_one_to_many(result_):\n",
    "    out = []\n",
    "    truth = result_[\"history\"][:,0]\n",
    "    for column in range(1, result_[\"history\"][:,1:].shape[1]+1):\n",
    "        epoch_result = result_[\"history\"][:,column]\n",
    "        out.append(rsquared(epoch_result, truth))\n",
    "    print(max(out))\n",
    "    return out\n",
    "\n",
    "def mse_one_to_many(result_):\n",
    "    out = []\n",
    "    truth = result_[\"history\"][:,0]\n",
    "    for column in range(1, result_[\"history\"][:,1:].shape[1]+1):\n",
    "        epoch_result = result_[\"history\"][:,column]\n",
    "        out.append(mean_squared_error(epoch_result, truth))\n",
    "    print(min(out))\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
